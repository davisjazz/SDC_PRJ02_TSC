{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Writeup | Self-Driving Car project - Deep Learning**\n",
    "**x** min read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract — This notebook is the writeup of the Traffic Sign Recognition project.** We apply Deep Learning and Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the SELF-DRIVING CAR nanodegree program. The project is broken down into three steps, which are:   \n",
    ">- Step 1: Data Set Summary and Exploration\n",
    "- Step 2: Design and Test a Model Architecture\n",
    "- Step 3: Test a Model on New Images\n",
    "\n",
    "The model yielded the accuracy of **97.7%** with a loss of **0.141** using 32x32 pre-proceeded input images.\n",
    "\n",
    "Here is the link to the [PROJECT SPECIFICATION](https://review.udacity.com/#!/rubrics/481/view), here to my [PROJECT CODE](https://github.com/chatmoon/Traffic-Sign-Classifier-Project/blob/master/_2_WIP/_JNBK_/_TSC-step2.1_170309-1557_WIP.ipynb) and here the [Table of Contents](https://github.com/chatmoon/Debugging/blob/master/_WRITEUP_TableOfContents_171104-1145.ipynb) of this Writeup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Data Set Summary & Exploration\n",
    "\n",
    "The goals here are the following:\n",
    "* Load the [data set](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) and Summarize\n",
    "* Explore and visualize the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Load and summarize the data set  \n",
    "\n",
    "*The code for this part is between the 1st and 4th cell of the IPython notebook.*   \n",
    "\n",
    "I used the numpy library to calculate summary statistics of the traffic signs data set:   \n",
    "* The size of training set is 34799\n",
    "* The size of validing set is 4410\n",
    "* The size of test set is 12630\n",
    "* The shape of a traffic sign image is (32, 32, 3)\n",
    "* The number of unique classes/labels in the data set is 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. An exploratory visualization of the dataset\n",
    "##### 1.2.1. Explore the data set\n",
    "\n",
    "*The code for this part is between the 5th and 11th cell of the IPython notebook.*   \n",
    "\n",
    "In this part, we have three representations of the data set:   \n",
    "- fig.1: a list showing the number of occurence per traffic sign name\n",
    "- fig.2: a bar chart showing the number of occurence per class id\n",
    "- fig.3: a bar chart showing the distribution of these traffic sign images into the data set\n",
    "\n",
    ">![fig.1](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataExplo_1_showList.PNG)\n",
    ">![fig.2](https://raw.githubusercontent.com/chatmoon/Debugging/master/_images_/_dataExplo_2_showChart.png)\n",
    ">![fig.3](https://raw.githubusercontent.com/chatmoon/Debugging/master/_images_/_dataExplo_3_showDist.png)\n",
    "\n",
    "**Observations:**    \n",
    "- The fig.1 and 2 show there is a large disparity between traffic sign occurences.   \n",
    "Additional data should be created in order to rebalance the under represented classes.\n",
    "- The fig.3 shows that each class has been piled on top of the other.   \n",
    "The data set should be shuffled before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Visualize the data set\n",
    "*The code for this part is between the 12th and 16th cell of the IPython notebook.*\n",
    "\n",
    "In this part, we have also three types of visualization of the data set:   \n",
    "- fig.4: 43 random images, one per class and with their black-boxes\n",
    "- fig.5: 5 x 10, 5 random traffic signs, 10 images each \n",
    "- fig.6: a sprite image showing all the traffic sign in a single image\n",
    "\n",
    "> *fig.4: 43 random images, one per class and with their black-box*\n",
    "![fig.4](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataVisu_4_show43TS.png)\n",
    "> *fig.5: 5 x 10, 5 random traffic signs, 10 images each*\n",
    "![fig.5](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataVisu_5_show5TS.png)\n",
    "> *fig.6: a sprite image showing all the traffic sign in a single image*\n",
    "![fig.6](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_sp_xx_5984x5984.png)   \n",
    "> *fig.7: a sampling of challenging images to classify*\n",
    "![fig.7](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataExplo_4_challenges.PNG)\n",
    "\n",
    "**Observations:**   \n",
    "The fig.4 to 7 show how the images are of different qualities. Many images are either shaky, too dark or not having the same scale. There are variabilities such as viewpoint variations, lighting conditions (saturations, low-contrast), motion-blur, occlusions, sun glare, physical damage, colors fading. The classification of the traffic sign can be challenging and complex in this context. We should pre-proceed the images to decrease the impact of the mixed qualities of images.\n",
    "\n",
    "**Note:** the sprite image will be useful for the embedding visualization in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Design and Test a Model Architecture\n",
    "\n",
    "#### 2.1. Preprocess the data set and generate additional data\n",
    "\n",
    "##### 2.1.1. Preprocess the data set\n",
    "*The code for this part is between the 18th and 21st cell of the IPython notebook, and 29th and 30rd cell.*\n",
    "\n",
    "> **Definition:** Pre-processing refers to techniques such as converting to grayscale, normalization, etc.\n",
    "\n",
    "In this part, I describe how I preprocessed the image data, what techniques were chosen and why I chose these techniques. You will also find below a overview of the preprocessing workflow in figure *fig.8* with images showing the output of each preprocessing technique.\n",
    "\n",
    "In summary, the preprocessing workflow generate five different types of image: 0RGB, 1GRAY, 2SHP, 3HST, 4CLAHE.\n",
    "\n",
    "> *fig.8: the preprocessing workflow*\n",
    "![fig.8](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataPPro%20flow_170610-1221.png)\n",
    "\n",
    "\n",
    "First of all, **all preprocessed images are centered and normalized** because during the training of the network we multiply weights to the initial input and add biases to cause activations and then backpropagate with the gradients to train (update) the model. In this process, we do not want the gradients go out of control. Then all preprocessed images are centered around zero by subtracting the mean, and normalized by dividing by the standard deviation. This technique doesn't change the content of the image. It avoids the values of weights and biases to get too big or to small. It tackles the numerical stability issue that occurs when several small values are added to big values (introducing a lot of errors) during the optimization of the Loss function.\n",
    "\n",
    "The two starting points of this approch are:\n",
    "- the following comments: \"the ConvNet was trained with full supervision on the colorimages of the GTSRB dataset and reached 98.97% accuracyon the phase 1 test set. After the end of phase 1, additional experiments with grayscale images established a new record accuracy of 99.17%\", Traffic Sign Recognition with Multi-Scale Convolutional Networks, Pierre Sermanet and Yann LeCun\n",
    "- the observation of the training set samples shows there are variabilities such as colors fading, lighting conditions (saturations, low-contrast), sun glare, motion-blur. To overcome a part of these variabilities and make the classification easier, I converted RGB images to grascale, then grayscale images have been sharpened, the histograms of sharpened images have been equilized, and the histograms of equilized images have been equilized adaptively with limited contrast\n",
    "\n",
    "The following functions from OpenCV have been used:\n",
    "- 1GRAY: [cvtColor](http://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html#cvtcolor)   \n",
    "- 2SHP: [filter2D](http://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#filter2d)   \n",
    "- 3HST: [equalizeHist](http://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html#equalizehist) to improve the contrast   \n",
    "- 4CLAHE: [createCLAHE](http://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html)   \n",
    "\n",
    "**Note:**\n",
    "- In addition, I also tried to blur the images but I got a better accuracy removing this last technic   \n",
    "- other realistic perturbations would probably also increase robustness of the model such as other affine transformations, brightness or adding some artificial occlusions. They would be implemented in the future sprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2. Generate additional data\n",
    "*The code for this part is between the 22nd and 28th cell of the IPython notebook, and 31st and 35th cell.*\n",
    "\n",
    "In this part, I describe how and why I generated additional data, what techniques were chosen and why I chose these techniques. You will also find below a workflow of the generation of additional data in figure fig.10 with a visualization of the jittered image in fig.9.\n",
    "\n",
    "I decided to generate additional data for two reasons:\n",
    "- first, the fig.2 shows the data set is imbalanced, i.e. the classes are not represented equally. In this case, the accuracy measures might be excellent accuracy on paper but it is only reflecting the underlying class distribution. For example, if the accuracy is 90% of the instances in Class-3 (Speed limit 50km/h) is because the models look at the data and cleverly decide that the best thing to do is to always predict “Class-3” and achieve high accuracy\n",
    "- secondly, the amount of data might be not sufficient for the model to generalise well in production with new data   \n",
    "\n",
    "\n",
    "To add more data to the the data set, I combined the two following techniques:\n",
    "- images are randomly picked and perturbed in position ([-2,2] pixels)\n",
    "- then they are pertubed in rotation ([-15,+15] degrees)   \n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- in addition, I also tried to use the bounding box to crop the images and then perturbing them in scale ([.9,1.1] ratio). I got an accuracy around 93%, far below the human performance of 98.81%. I removed this last part and I get a better result at the end      \n",
    "- the observation of the training set in fig.6 shows the data set is a stack of several series of 30 similar images **with usually increasing scale**. It is for that reason I did not implement the simple perturbation in scale\n",
    "- there are 21 traffic signs that have a horizontal or a vertical axis of symmetry. Consequently, they are invariant to horizontal or vertical flipping. This technic would be implemented to add more data to the data set in the future sprint\n",
    "\n",
    "Here is an example of an original image and an augmented image:\n",
    "> *fig.9: augmented data example*\n",
    "![fig.9](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataJit_test4.png)\n",
    "\n",
    "I did not know how much the data would have to be raised to improve the accuracy or to overcome the overfitting problem. Then I created several set of augmented data such as each class has a least the following quantity (qty) of occurence: 500, 1000, 1500, 2000, 2500 and 3000 for each preprocessed type of image (see fig.10).\n",
    "> *fig.10: workflow of the generation of additional data*\n",
    "![fig.10](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataJIT_flow_170617-2336.png)   \n",
    "> *tab.11: the size of augmented training set*\n",
    "\n",
    "| Minimum quantity per class | 0 | 500 | 1000 | 1500 | 2000 | 2500 | 3000 |\n",
    "|:---------------------------------------------------------------:| \n",
    "| Quantity in addion | 0 | 4440 | 12451 | 15690 | 18630 | 21490 | 21500 |\n",
    "| Total size | 34799 | 39239 | 51690 | 67380 | 86010 | 107500 | 129000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Model Architecture   \n",
    "*The code for this part is between the 36th and 44th cell of the IPython notebook.*   \n",
    "\n",
    "In this part, I describe what the final model architecture used looks like, how the model has been trained and the approach taken for finding a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1. The final model architecture\n",
    "\n",
    "*The code for this part is in the 44th cell of the IPython notebook.*   \n",
    "\n",
    "Even though I have built and experimented [several models](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428.png), I get the best test and validation accuracies with the Lenet5 architecture.\n",
    "\n",
    "> *fig.12: the original LeNet5 architecture*\n",
    "![fig.12](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/lenet5.png)\n",
    "\n",
    "Here is a diagram describing the final model:\n",
    "> *fig.13: the final model architecture used*\n",
    "![fig.13](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428_mod1.png)\n",
    "\n",
    "The model consisted of the following layers:   \n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t | \n",
    "|:---------------------:|:----------------------------------------------:| \n",
    "| Input         \t\t| 32x32xch, ch=3 for RGB image or 1 for grayscale| \n",
    "| Convolution        \t| 1x1 stride, valid padding, outputs 28x28x6   \t |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 14x14x6  \t\t\t\t |\n",
    "| Convolution   \t    | 1x1 stride, valid padding, outputs 10x10x16\t |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 5x5x16  \t\t\t\t     |\n",
    "| Flatten\t      \t\t| outputs 400  \t\t\t\t    \t\t\t     |\n",
    "| Fully connected\t\t| outputs 200  \t\t\t\t    \t\t\t     |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Dropout\t\t\t\t| keep_prob = 0.67\t\t\t\t\t\t\t\t |\n",
    "| Fully connected\t\t| outputs 84  \t\t\t\t    \t\t\t     |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Dropout\t\t\t\t| keep_prob = 0.67\t\t\t\t\t\t\t\t |\n",
    "| Fully connected\t\t| outputs 43  \t\t\t\t    \t\t\t     |\n",
    "\n",
    "\n",
    "**Note:** The graph of the model can be found using Tensorboard: [graph](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_tensorboard.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2. How the model has been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code for this part is between the 46th and 51st cell of the IPython notebook.*\n",
    "\n",
    "In this part, the discussion includes the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.\n",
    "\n",
    "To train the model, I used the Adam optimyzer. Adam stands for Adaptive moment estimation. It usually outperforms the other types of optimizer like the gradient descent, the stochastic gradient descent, the mini-batch gradient descent, the momentum or the Nesterov accelerated gradient. For more details, see this great short [video](https://www.youtube.com/watch?v=nhqo0u1a6fw).   \n",
    "\n",
    "I used ReLU as activation function for the hidden layers. I implemented and tested leaky ReLU but the ReLU function gave me the best validation and test accuracies results. It outperforms the other activation functions like Sigmoid and hyperbolic tangent function (tanh). I did not try Maxout function. For more details, see this short [video](https://www.youtube.com/watch?v=-7scQpJT7uo).\n",
    "\n",
    "I used the dropout technique as regularization method with a rate of 0.67.\n",
    "\n",
    "Regarding the other hyperparameters:   \n",
    "\n",
    "| Hyperparameter    \t| Value  |   \n",
    "|:---------------------:|:------:|   \n",
    "| LEARNING RATE       \t| 8.5e-4 |   \n",
    "| EPOCHS   \t            | 100\t |   \n",
    "| BATCH SIZE            | 100\t |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3. The approach taken for finding a solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.0. Description of the general approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was an iterative approach.   \n",
    "\n",
    "As a prelude, I used the Lenet5 architecture with the initial RGB images as input data and arbitrary hyperparameter values.   \n",
    "I chose this starting point for two reasons:   \n",
    "- Firstly, the initial measure gave me a benchmark value of the validation accuracy that would serve as an element of comparaison when optimizing the architecture performance. At this stage, I got a validation accuracy of 91.7%, a test accuracy of 90.3% and a loss of 0.456 with a learning rate of 1E-3.  \n",
    "- Secondly, the Lenet5 architecture is a building block of the Multi-Scale Convolutional Network. ![ MultiScaleConvNet](https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/9ab0de951cc9cdf16887b1f841f8da6affc9c0de/1-Figure2-1.png)   \n",
    "The latter promises a validation accuracy of 99.17%, above the human performance of 98.81%. In this way, I was able to get a sense of the impact of each parameter on a simpler system before playing with more complex architectures.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I played with the following parameters to tune the Lenet5 model:   \n",
    "- learning rate: [1E-3, 2E-3, 9E-4, 1E-4, 1E-5, 8E-4, 9.5E-4, 8.5E-4]\n",
    "- dropout rate: [0.5, 0.75, 0.25, 0.85, 0.6, 0.8, 0.67]\n",
    "- preprocessed data types: (centered & normalized) AND [RGB, grayscale, sharpen, histogram, CLAHE]\n",
    "- the amount of jittered data: [500, 1000, 1500, 2000, 2500, 3000]   \n",
    "- activation function: [Relu, leakyRelu]\n",
    "\n",
    "As a result, I got a better performance: a validation accuracy of 97.7%, a test accuracy of 95.8% with a loss of 0.141.\n",
    "\n",
    "Finally, I played with various changes in the initial architecture and tried other ones.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.1. Initial benchmark values with the Lenet5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code for this part is between the 52nd and 55th cell of the IPython notebook.*\n",
    "\n",
    "At this stage, I got a validation accuracy of 91.7%, a test accuracy of 90.3% and a loss of 0.456 with a learning rate of 1E-3.   \n",
    "\n",
    "The fig.14.a shows the model does not overfit or underfit.     \n",
    "\n",
    "> *fig.14.a: model1 - cost and accuracies measures before, in between and tuning*\n",
    "![fig.14.a](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/4521055bf8aaa1f74f97d2eb83f0853ac51567f3/images/_TensorBoard_Mod1_variousLearningRate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.2. Tuning of the Lenet5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the learning rate, I was able to get a validation accuracy of 94.4% with a learning rate of 8.5E-4 *(see fig. 15.a)*.   \n",
    "\n",
    "A learning rate of 8.5E-4 would be kept for the next measures.\n",
    "\n",
    "> *fig.15.a: model1's cost and accuracies measures with various learning rates*\n",
    "![fig.15.a]( https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousLearningRate.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_171029-1039.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the dropout rate, I got better performance: a validation accuracy of 94.7% and a test accuracy of 93.5% with a dropout rate of 0.67 *(see fig. 15.b)*.   \n",
    "\n",
    "> *fig.15.b: model1's cost and accuracies measures with various dropout rates*\n",
    "![fig.15.b]( https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousDropoutRates.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_171029-1039.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the preprocessed data types in input, I got an improvment using centered and normalized data, and with grayscale and RGB images *(see fig. 16)*:\n",
    "- grayscale & centered & normalized: validation accuracy of 96.3%, a test accuracy of 94.5%, a cost of 0.279   \n",
    "- RGB & centered & normalized: validation accuracy of 94.9%, a test accuracy of 94.7%, a cost of 0.492   \n",
    "\n",
    "> *fig.16: model1's cost and accuracies measures with various with various preprocessed data types*\n",
    "![fig.16](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousPproDataType.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_171029-1039.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the amount of jittered data in input, I got an improvment using at least 3000 centered and normalized RGB images per class: a validation accuracy of 97.7%, a test accuracy of 95.8%, a cost of 0.141 *(see fig. 17)*.   \n",
    "\n",
    "> *fig.17: model1's cost and accuracies measures with various with various amount of jittered grayscale & RGB data*\n",
    "![fig.17](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousJitData.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_171029-1039.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.3. Other architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by varying architectures, the Lenet5 architecture has still gotten the best performance results *(see fig.18, .19)*.   \n",
    "\n",
    "> *fig.18: cost and accuracies measures with various architectures*\n",
    "![fig.18](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterModXNew_result.png)\n",
    "*Find [here](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428.png) the details of each model.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *fig.19: different architectures descriptiion that have been tested*\n",
    "![fig.19](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1429.PNG)\n",
    "*Find [here](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428.png) the details of each model.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.4. In conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must say that I am a bit desappointed. Even though I implemented the Multi-Scale Convolutional Network and used various pre-processing data techniques, the performance was not as good as the record of 99.17% promised by the paper. I expected to improve the performance by playing with various architectures but I was not successful despite my several attempts and due to some hardware limitions. Only the standardization, normalization and jitteration techniques have a significant impact on the validation and test accuracies. \n",
    "\n",
    "In conclusion, after building and experimenting seven models, the best validation and test accuracies have been achieved with the Lenet5 architecture, using 32x32x3, jittered, centered and normalized RGB images. **The model yielded a validation accuracy of 97.7%, a test accuracy of 95.8% with a loss of 0.141.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Test a Model on New Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I tested the model with new images. These latter were taken on German streets or they were screenshots of street view from Google Map.\n",
    "\n",
    "If you look at fig.4, you will find there are five general shapes that describe the 43 traffic signs. These genral shapes are shown in the fig.20 below:\n",
    "> *fig.20: five general shapes describing the 43 traffic signs*\n",
    "![fig.20](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_Step3_/_TS_5_general_shapes.PNG)\n",
    "\n",
    "Then I created three series of images:\n",
    "- serie 01: it contents only five traffic signs with shape 1, cercle\n",
    "- serie 02: it contents only five traffic signs with shape 2, triangle pointing upward\n",
    "- serie 03: it contents five traffic signs with only one shape from any shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Load and Output the Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code for this part is between the 56th and 59th cell of the IPython notebook.*\n",
    "\n",
    "The fig.21.1 shows the five German traffic signs of serie 01\n",
    "- the first line shows the original images\n",
    "- the second line shows RGB images that were centered and normalized\n",
    "- the thrid line shows grayscale images that were also centered and normalized\n",
    "> *fig.21.1: serie 1 - five new German traffic signs*\n",
    "![fig.21.1](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_Step3_/_serie01_/_ownImages_serie01_0_ori-rgb-gray.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fig.21.2 shows the five German traffic signs of serie 02\n",
    "> *fig.21.2: serie 2 - five new German traffic signs*\n",
    "![fig.21.2](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_Step3_/_serie02_/_ownImages_serie02_0_ori-rgb-gray.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fig.21.3 shows the five German traffic signs of serie 03\n",
    "> *fig.21.3: serie 3 - five new German traffic signs*\n",
    "![fig.21.3](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_Step3_/_serie03_/_ownImages_serie03_0_ori-rgb-gray.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**   \n",
    "The images are of good qualities. They have similar viewpoint orientation. They are not shaky, too dark or having different scale. They don't have major imperfections such as motion-blur, occlusions (except a tiny occlusion on the first image), sun glare or physical damage. But there are variabilities such as lighting conditions (saturations, low-contrast) and about the background contents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. The model's predictions on these new traffic signs   \n",
    "\n",
    "*The code for this part is between the 61st and 66th cell of the IPython notebook.*\n",
    "\n",
    "The model was able to correctly guess **0 of the 5 traffic signs**, which gives **an accuracy of 0%**.   \n",
    "\n",
    "The fig.22 shows the predictions on the serie01 images in grayscale.\n",
    "> *fig.22: serie 1 - predictions on the grayscale images*\n",
    "![fig.22](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_Step3_/_serie01_/_ownImages_serie01_gray_0_prediction.PNG)   \n",
    "\n",
    "| Image\t\t\t        |     Prediction    | \n",
    "|:---------------------:|:-----------------:| \n",
    "| Speed limit (20km)\t| Double curve      | \n",
    "| Speed limit (30km)\t| Speed limit (70km)|\n",
    "| Speed limit (50km)\t| Speed limit (70km)|\n",
    "| End of all speed   \t| Pedestrians       | \n",
    "| Ahead only\t\t\t| Speed limit (70km)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model recognized none of them. Furthermore, five circle shapes have been given as an input. The prediction managed to recognized two triangles. How is it possible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Having come a long way, I don't get it why the model fails to generalize with these images. I am on the very verge of throwing my laptop through the window and then myself... Fortunately I live on the first floor.    \n",
    "\n",
    "Having said that, I tried to run the prediction on images from the training, validation and test data set. And again, the model yielded a validation accuracy of 0%. Why that? How could the model yield a test accuracy of 95% and then it fails to recognize images from the training, validation and test data set?   \n",
    "\n",
    "In addition, the prediction results are not repeatable. Each time I run prediction with the same input images, I get different results.\n",
    "\n",
    "What can I do to analyze the code and to find out the problem?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.3. The top 5 softmax probabilities and certainty of the model's prediction  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code for this part is between the 67th and 73th cell of the IPython notebook.*\n",
    "\n",
    "In this part, we describe how certain the model is when predicting on each of the new images by looking at the softmax probabilities for each prediction.\n",
    "\n",
    "The fig.23.1 shows the top five predictions on the serie01 images in grayscale.   \n",
    "The fig.23.2 shows the visualization of predictions as bar chart.   \n",
    "And the fig.23.3 replaces the bar chat by an image with the prediction as a title.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "> *fig.23.1: serie 1 - predictions on the grayscale images*\n",
    "![fig.23.1](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_Step3_/_serie01_/_ownImages_serie01_gray_3_topK_image.png)   \n",
    "> *fig.23.2: serie 1 - visualization of predictions as bar chart*    \n",
    "![fig.23.2](https://raw.githubusercontent.com/chatmoon/Debugging/master/_images_/_Step3_/_serie01_/_ownImages_serie01_gray_2_topK43_chartBar.png)   \n",
    "> *fig.23.3: serie 1 - visualization of predictions*    \n",
    "![fig.23.3](https://raw.githubusercontent.com/chatmoon/Debugging/master/_images_/_Step3_/_serie01_/_ownImages_serie01_gray_3_topK35_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fig.23.1 shows the top five predictions for each images vary between 3% to 5%. How could the top predictions be so low? Namely, for the three speed limits images, the model hesitates between one circle and four triangles. Is the prediction methode or the data processing or the training process that should be analyzed to solve this issue? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fig.23.2 and fig.23.3 shows a visualization of the predictions.   \n",
    "For the first image, the right anwser is ranked in the 13th position.   \n",
    "For the second image, it is ranked in the second postion.   \n",
    "For the third image, it is ranked in the 11th position.   \n",
    "For the fourth image, it is ranked in the 35th position.   \n",
    "Finally, for the fifth image, it is ranked in the 34th position.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[writeup_template.md](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
